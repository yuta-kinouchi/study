diff --git a/iee/function/single.py b/iee/function/single.py
index 99e73a7..9242a4f 100644
--- a/iee/function/single.py
+++ b/iee/function/single.py
@@ -6,185 +6,352 @@ import matplotlib.pyplot as plt
 import traci
 from collections import defaultdict
 import pickle
+import scipy.linalg as LA
 
-# Qtable = np.zeros((120,50,120,50,8,4))
-Qtable = np.zeros((150,150,2,2))
-epsilon = 0.1
-gamma = 0.9
-alpha = 0.1
-xmlfl = "reinforcement.net.xml"
-car_id = {}
-car_id_2 = {}
-car_id = defaultdict(list)
-car_id_2 = defaultdict(list)
-
-np.set_printoptions(threshold=np.inf)
-
-def count_traveltime(id,cycle):
-    for i in id:
-        if i not in car_id or car_id[i][1] != cycle:
-            car_id[i] =[0, cycle]
+def rulebase():
+    car_id = {}
+    car_id = defaultdict(list)
+    def count_traveltime(id,cycle):
+        for i in id:
+            # そのIDがリストの中にあるかないかチェック
+            if i not in car_id or car_id[i][1] != cycle:
+                car_id[i] =[0, cycle]
+                
+            else:
+                car_id[i][0] += 1
             # print(car_id[i])
+            # print(car_id[i][0])
+
+    rewards = []
+    travel_time = []
+    # traci.start(["sumo-gui", "-c", "single.sumocfg"])
+    traci.start(["sumo", "-c", "single.sumocfg"])
+    # traci.simulationStep(1000)
+    cycle = 0
+    j = 0
+    for iter in range(10):
+        for step in range(1000):
+            # r = reward()
+            # rewards.append(r)
+            id = traci.vehicle.getIDList()
+            # check_car()
+            count_traveltime(id,cycle)
+            traci.simulationStep()
+        
+        if j == 0:
+            j += 1
         else:
-            car_id[i][0] += 1
-            # print(car_id[i])
+            a = 0
+            b = 1
+            for i in car_id:
+                if car_id[i][1] == cycle:
+                    a += car_id[i][0]
+                    b += 1
+            traveltime = a / b
+            # print(traveltime)
+            travel_time.append(traveltime)
+            # print(travel_time)
+            cycle += 1
+    traci.close()
+    return travel_time
 
-def count_traveltime_2(id,cycle):
-    for i in id:
-        if i not in car_id_2 or car_id_2[i][1] != cycle:
-            car_id_2[i] =[0, cycle]
-            # print(car_id[i])
+def reinforcement():
+
+    # Qtable = np.zeros((150,150,2,2))
+    # epsilon = 0.1
+    gamma = 0.9
+    alpha = 0.1
+    xmlfl = "reinforcement.net.xml"
+    car_id = {}
+    car_id = defaultdict(list)
+
+
+    def count_traveltime(id,cycle):
+        for i in id:
+            if i not in car_id or car_id[i][1] != cycle:
+                car_id[i] =[0, cycle]
+                # print(car_id[i])
+            else:
+                car_id[i][0] += 1
+                # print(car_id[i])
+
+    def get_state():
+        cars_r_c = traci.edge.getLastStepVehicleNumber("r_c")
+        cars_l_c = traci.edge.getLastStepVehicleNumber("l_c")
+        cars_t_c = traci.edge.getLastStepVehicleNumber("t_c")
+        cars_b_c = traci.edge.getLastStepVehicleNumber("b_c")
+        tate = (cars_t_c + cars_b_c)
+        yoko = (cars_r_c + cars_l_c)
+        phase = traci.trafficlight.getPhase("c")
+        if phase < 4:
+            phase = 0
         else:
-            car_id_2[i][0] += 1
-            # print(car_id[i])
+            phase = 1
+        state = [tate,yoko,phase]
+        return state
 
-def get_state():
-    cars_r_c = traci.edge.getLastStepHaltingNumber("r_c")
-    # cars_rr_r = traci.edge.getLastStepVehicleNumber("rr_r")
-    # cars_r_c_left = traci.lane.getLastStepVehicleNumber("r_c_2")
-    # cars_r_c = cars_r_c - cars_r_c_left
+    def state_trans(a):
+        phase = traci.trafficlight.getPhase("c")
+        if a == 0:
+            if phase == 0:
+                traci.trafficlight.setPhase("c",0)
+            else:
+                traci.trafficlight.setPhase("c",3)
+        elif a == 1:
+            if phase == 2:
+                traci.trafficlight.setPhase("c",2)
+            else:
+                traci.trafficlight.setPhase("c",1)
+
+    def reward():
+        t_c = traci.edge.getLastStepHaltingNumber("t_c")
+        r_c = traci.edge.getLastStepHaltingNumber("r_c")
+        l_c = traci.edge.getLastStepHaltingNumber("l_c")
+        b_c = traci.edge.getLastStepHaltingNumber("b_c")
+        r =  -(t_c + r_c + l_c + b_c)/300
+        # r =  -(t_c + r_c + l_c + b_c)
+        return r
+
+    # def get_Qvalue(s,a):
+    #     return Qtable[s[0]][s[1]][s[2]][a]
     
-    cars_l_c = traci.edge.getLastStepHaltingNumber("l_c")
-    # cars_ll_l = traci.edge.getLastStepVehicleNumber("ll_l")
-    # cars_l_c_left = traci.lane.getLastStepVehicleNumber("l_c_2")
-    # cars_l_c = cars_l_c - cars_l_c_left
-
-    cars_t_c = traci.edge.getLastStepHaltingNumber("t_c")
-    # cars_tt_t = traci.edge.getLastStepVehicleNumber("tt_t")
-    # cars_t_c_left = traci.lane.getLastStepVehicleNumber("t_c_2")
-    # cars_t_c = cars_t_c - cars_t_c_left
-
-    cars_b_c = traci.edge.getLastStepHaltingNumber("b_c")
-    # cars_bb_b = traci.edge.getLastStepVehicleNumber("bb_b")
-    # cars_b_c_left = traci.lane.getLastStepVehicleNumber("b_c_2")
-    # cars_b_c = cars_b_c - cars_b_c_left
-
-
-    # yoko = cars_t_c + cars_tt_t + cars_b_c + cars_bb_b
-    # tate = cars_r_c + cars_rr_r + cars_l_c + cars_ll_l
-    tate = (cars_t_c + cars_b_c)
-    yoko = (cars_r_c + cars_l_c)
-    # tate_left = (cars_r_c_left + cars_l_c_left)
-    # yoko_left = (cars_t_c_left + cars_b_c_left)
-    phase = traci.trafficlight.getPhase("c")
-    if phase < 4:
-        phase = 0
-    else:
-        phase = 1
-
-    # state = [tate,tate_left,yoko,yoko_left,phase]
-    state = [tate,yoko,phase]
-    return state
-
-def select_action(s):
-    if epsilon < np.random.rand():
-        # print(s[2])
-        q = Qtable[s[0]][s[1]][s[2]]
-        actions = np.where(q == q.max())[0]
-        return np.random.choice(actions)
-    else:
-        return np.random.choice(2)
-
-def select_greedy_action(s):
-    q = Qtable[s[0]][s[1]][s[2]]
-    actions = np.where(q == q.max())[0]
-    return np.random.choice(actions)
-
-
-def state_trans(a):
-    phase = traci.trafficlight.getPhase("c")
-    # print("action:{0}".format(a))
-    # print("phase:{0}".format(phase))
-    if a == 0:
-        if phase == 0:
-            traci.trafficlight.setPhase("c",0)
-            # print("1")
-        else:
-            traci.trafficlight.setPhase("c",5)
-            # print("2")
-    elif a == 1:
-        if phase == 4:
-            traci.trafficlight.setPhase("c",4)
-            # print("1")
-        else:
-            traci.trafficlight.setPhase("c",1)
-            # print("2")
+    class LinearFunc:
+        N1 = 2
+        N2 = 2
+        N3 = 2
+        # eps = 0.1
+        gamma = 0.9
+        alpha = 0.1
+        var = 0.2
+        ts = 3
+        NUM_ACTION = 2
+        myu_dist_wid = np.linspace(0, N1-1, N1)
+        myu_dist_hei = np.linspace(0, N2-1, N2)
+        myu_dist_hig = np.linspace(0, N3-1, N3)
 
+        #　正規化
+        norm_factor = np.array([N1-1, N2-1, N3-1])
+        norm_factor = norm_factor.reshape(len(norm_factor),1)
 
+        def __init__(self):
+            self.N1_space = self.myu_dist_wid
+            self.N2_space = self.myu_dist_hei
+            self.N3_space = self.myu_dist_hig
+            
+            b = (self.N1*self.N2*self.N3) # 状態空間を分割した場合の総数 = 基底関数の総数
+                    
+            # 基底関数の定数項を初期化（学習対象外）
+            self.mu_array = np.zeros((self.ts, b)) # 分布の中心値をオール1に初期化
+            
+            cnt =0
+            # このN1、N2の順番に注意
+            for i in self.N3_space:
+                for j in self.N2_space:
+                    for k in self.N1_space:
+                        self.mu_array[0,cnt] = i
+                        self.mu_array[1,cnt] = j
+                        self.mu_array[2,cnt] = k
+                        cnt+=1
+            
+            # エピソード数、はparamsでセット
+            #self.episodes = episodes
+            
+            # 4つのアクションに対して、同じ基底関数セットを用いる
+            self.mu_list = [np.copy(self.mu_array)] * self.NUM_ACTION
+            # print(self.mu_list)
+            
+            # 学習対象のパラメータθの初期化
+            self.omega_list = []
+            for i in range(2):
+                self.omega_list.append(np.zeros(b))
 
+        def rbfs(self, s):
+            # 入力3次元、出力2行動
+            s = s.reshape(3,1) # 2次元配列に整形
+            return np.exp(-np.square(LA.norm((self.mu_list-s)/self.norm_factor, axis=1))/(2*self.var)) 
+    
+        def getQ(self, s, a):
+            return (self.rbfs(s)[a]).dot(self.omega_list[a])
 
-def reward():
-    t_c = traci.edge.getLastStepHaltingNumber("t_c")
-    # tt_t = traci.edge.getLastStepHaltingNumber("tt_t")
-    r_c = traci.edge.getLastStepHaltingNumber("r_c")
-    # rr_r = traci.edge.getLastStepHaltingNumber("rr_r")
-    l_c = traci.edge.getLastStepHaltingNumber("l_c")
-    # ll_l = traci.edge.getLastStepHaltingNumber("ll_l")
-    b_c = traci.edge.getLastStepHaltingNumber("b_c")
-    # bb_b = traci.edge.getLastStepHaltingNumber("bb_b")
-    r =  -(t_c + r_c + l_c + b_c)/300
-    # r =  -(t_c + tt_t + r_c + rr_r + l_c + ll_l + b_c + bb_b)
-    return r
+        def select_action(self, s,episode):
+            # e-greedyによる行動選択  
+            epsilon = 0.001 + 0.9 / (1.0+episode)
+            if np.random.rand() < epsilon:
+                action = np.random.randint(self.NUM_ACTION)
+                # print("A")
+                # print(action)
+                return action
+            else:
+                qs = [self.getQ(s,i) for i in range(self.NUM_ACTION)]
+                action = np.argmax(qs)
+                # 最大値をとる行動が複数ある場合はさらにランダムに選択
+                is_greedy_index = np.where(qs == action)[0]
+                if len(is_greedy_index) > 1:
+                    action = np.random.choice(is_greedy_index)
+                # print("B")
+                # print(action)
+                return action
 
-def get_Qvalue(s,a):
-    return Qtable[s[0]][s[1]][s[2]][a]
+        def train(self,before_s,before_a,r,s,a):
+            phi = self.rbfs(before_s)
+            Qvalue = self.getQ(before_s,before_a)
+            Qvalue_dash = self.getQ(s,a)
+            DELTA = r + self.gamma*Qvalue_dash - Qvalue
+            self.omega_list[before_a] = self.omega_list[before_a] + self.alpha * DELTA * phi[before_a]
 
-if __name__ =="__main__":
 
-    # os.chdir(os.path.dirname(os.path.abspath(__file__)))    
-    # rewards = []
-    # travel_time_2 = []
-    # # traci.start(["sumo-gui", "-c", "rulebase.sumocfg"])
-    # traci.start(["sumo", "-c", "rulebase.sumocfg"])
-    # traci.simulationStep(1000)
-    # cycle = 0
-    # z = 0
-    # for iter in range(10):
-    #     for step in range(95):
-    #         # r = reward()
-    #         # rewards.append(r)
-    #         id = traci.vehicle.getIDList()
-    #         # check_car()
-    #         count_traveltime_2(id,cycle)
-    #         traci.simulationStep()
-
-    #     if iter % 10 == 0:
-    #         if z == 0:
-    #             z += 1
-    #         else:
-    #             a = 0
-    #             b = 1
-    #             for i in car_id_2:
-    #                 if car_id_2[i][1] == cycle:
-    #                     a += car_id_2[i][0]
-    #                     b += 1
-    #             traveltime = a / b
-    #             # print(traveltime)
-    #             travel_time_2.append(traveltime)
-    #             # travel_time_2[0] = None
-    #             # print(travel_time)
-    #             cycle += 1
-    # traci.close()
+    os.chdir(os.path.dirname(os.path.abspath(__file__)))    
+    rewards = []
+    travel_time = []
+    cycle = 0
+    traci.start(["sumo", "-c", "single.sumocfg"]) 
+    # traci.start(["sumo-gui", "-c", "single.sumocfg"]) 
+    # traci.simulationStep(2000)
+    s = get_state()
+    s = np.array(s)
+    a = 0
+    phase_before = 0
+    num = 0
+    agent = LinearFunc()
+    for episode in range(10):
+        for step in range(1000):
+            id = traci.vehicle.getIDList()
+            count_traveltime(id,cycle)
+            phase = traci.trafficlight.getPhase("c")
+            if phase_before == phase:
+                if num == 10:
+                    before_s = s
+                    s = get_state()
+                    s = np.array(s)
+                    before_a = a
+                    a = agent.select_action(s,episode)
+                    r = reward()
+                    agent.train(before_s,before_a,r,s,a)
+                    state_trans(a)
+                    phase_before = traci.trafficlight.getPhase("c")
+                    num = -1
+            else:
+                if num == 13:
+                    before_s = s
+                    s = get_state()
+                    s = np.array(s)
+                    before_a = a
+                    a = agent.select_action(s,episode)
+                    r = reward()
+                    agent.train(before_s,before_a,r,s,a)
+                    state_trans(a)
+                    phase_before = traci.trafficlight.getPhase("c")
+                    num = -1
+            num += 1
+            traci.simulationStep()
+
+        c = 0
+        b = 1
+        for i in car_id:
+            if car_id[i][1] == cycle:
+                c += car_id[i][0]
+                b += 1
+        traveltime = c / b
+        travel_time.append(traveltime)
+        # print(travel_time)
+        cycle += 1
+    traci.close()
+    return travel_time,
+        
+def reinforcement2():
+    Qtable = np.zeros((150,150,2,2))
+    epsilon = 0.1
+    gamma = 0.9
+    alpha = 0.1
+    xmlfl = "reinforcement.net.xml"
+    car_id = {}
+    car_id = defaultdict(list)
+
+
+    def count_traveltime(id,cycle):
+        for i in id:
+            if i not in car_id or car_id[i][1] != cycle:
+                car_id[i] =[0, cycle]
+                # print(car_id[i])
+            else:
+                car_id[i][0] += 1
+                # print(car_id[i])
+
+    def get_state():
+        cars_r_c = traci.edge.getLastStepVehicleNumber("r_c")
+        cars_l_c = traci.edge.getLastStepVehicleNumber("l_c")
+        cars_t_c = traci.edge.getLastStepVehicleNumber("t_c")
+        cars_b_c = traci.edge.getLastStepVehicleNumber("b_c")
+        tate = (cars_t_c + cars_b_c)
+        yoko = (cars_r_c + cars_l_c)
+        phase = traci.trafficlight.getPhase("c")
+        if phase < 4:
+            phase = 0
+        else:
+            phase = 1
+        state = [tate,yoko,phase]
+        return state
+
+    def select_action(s):
+        if epsilon < np.random.uniform(0,1):
+            # print(s[2])
+            q = Qtable[s[0]][s[1]][s[2]]
+            actions = np.where(q == q.max())[0]
+            return np.random.choice(actions)
+        else:
+            return np.random.choice(2)
+
+    def select_action(s):
+        q = Qtable[s[0]][s[1]][s[2]]
+        actions = np.where(q == q.max())[0]
+        return np.random.choice(actions)
+
+    def reward():
+        t_c = traci.edge.getLastStepHaltingNumber("t_c")
+        r_c = traci.edge.getLastStepHaltingNumber("r_c")
+        l_c = traci.edge.getLastStepHaltingNumber("l_c")
+        b_c = traci.edge.getLastStepHaltingNumber("b_c")
+        r =  -(t_c + r_c + l_c + b_c)/300
+        # r =  -(t_c + r_c + l_c + b_c)
+        return r
+
+    def state_trans(a):
+        phase = traci.trafficlight.getPhase("c")
+        # print("action:{0}".format(a))
+        # print("phase:{0}".format(phase))
+        if a == 0:
+            if phase == 0:
+                traci.trafficlight.setPhase("c",0)
+            else:
+                traci.trafficlight.setPhase("c",3)
+        elif a == 1:
+            if phase == 2:
+                traci.trafficlight.setPhase("c",2)
+            else:
+                traci.trafficlight.setPhase("c",1)
+
+    def get_Qvalue(s,a):
+        return Qtable[s[0]][s[1]][s[2]][a]
+
 
     os.chdir(os.path.dirname(os.path.abspath(__file__)))    
     rewards = []
     travel_time = []
     cycle = 0
-    traci.start(["sumo", "-c", "reinforcement.sumocfg"]) 
-    # traci.start(["sumo-gui", "-c", "reinforcement.sumocfg"]) 
+    traci.start(["sumo", "-c", "single.sumocfg"]) 
+    # traci.start(["sumo-gui", "-c", "single.sumocfg"]) 
     # traci.simulationStep(2000)
     s = get_state()
     a = 0
     phase_before = 0
     num = 0
-    for iter in range(2000):
+    for iter in range(10):
         for step in range(1000):
             id = traci.vehicle.getIDList()
             count_traveltime(id,cycle)
             phase = traci.trafficlight.getPhase("c")
-            # print(num)
             if phase_before == phase:
-                if num == 5:
+                if num == 10:
                     before_s = s
                     s = get_state()
                     before_a = a
@@ -195,9 +362,8 @@ if __name__ =="__main__":
                     state_trans(a)
                     phase_before = traci.trafficlight.getPhase("c")
                     num = -1
-                    
             else:
-                if num == 27:
+                if num == 13:
                     before_s = s
                     s = get_state()
                     before_a = a
@@ -208,7 +374,6 @@ if __name__ =="__main__":
                     state_trans(a)
                     phase_before = traci.trafficlight.getPhase("c")
                     num = -1
-                    
             num += 1
             traci.simulationStep()
 
@@ -222,21 +387,20 @@ if __name__ =="__main__":
         travel_time.append(traveltime)
         # print(travel_time)
         cycle += 1
+    traci.close()
+    return travel_time
+
+
+
 
-    # print("拡張子を除くフォルダ名")
-    # folder = input()
-    # folder = "./qtable/" + folder + ".txt"
-    # f = open(folder,"wb")
-    # pickle.dump(Qtable,f)
-    #plt.rcParams['font.family'] = "IPAexGothic"
-    # for t in Qtable:
-    #     print(t)
-    # print(Qtable)
-    plt.plot(travel_time)
-    # p1 = plt.plot(travel_time)
-    # p2 = plt.plot(travel_time_2)
-    # plt.legend((p1[0], p2[0]), ("reinforcement", "rulebase"), loc=2)
-    plt.xlabel("step(×1000)")
-    plt.ylabel("travel time")
-    plt.show()
-    # print(car_id)
+reinforcement = reinforcement()
+rulebase = rulebase()
+reinforcement2 = reinforcement2()
+p1 = plt.plot(rulebase)
+p2 = plt.plot(reinforcement)
+p3 = plt.plot(reinforcement2)
+plt.legend((p1[0],p2[0],p3[0]), ("rulebase","function","reinforcement"), loc=2)
+plt.xlabel("step(×1000)")
+plt.ylabel("travel time")
+plt.show()
+# print(car_id)
diff --git a/iee/function/single.rou.xml b/iee/function/single.rou.xml
index 9349eb3..48db142 100644
--- a/iee/function/single.rou.xml
+++ b/iee/function/single.rou.xml
@@ -64,10 +64,10 @@
 <flow id="type11" color="1,1,0" begin="0" end="60000" probability="0.025" type="car" route="l_b"/>
 <flow id="type12" color="1,1,0" begin="0" end="60000" probability="0.075" type="car" route="l_r"/> -->
 
-<flow id="type1" color="1,1,0" begin="0" end="60000" probability="0.1" type="car" route="b_t"/>
-<flow id="type2" color="1,1,0" begin="0" end="60000" probability="0.1" type="car" route="t_b"/>
-<flow id="type3" color="1,1,0" begin="0" end="60000" probability="0.1" type="car" route="l_r"/>
-<flow id="type4" color="1,1,0" begin="0" end="60000" probability="0.1" type="car" route="r_l"/>
+<flow id="type1" color="1,1,0" begin="0" end="60000" probability="0.3" type="car" route="b_t"/>
+<flow id="type2" color="1,1,0" begin="0" end="60000" probability="0.3" type="car" route="t_b"/>
+<!-- <flow id="type3" color="1,1,0" begin="0" end="60000" probability="0.05" type="car" route="l_r"/>
+<flow id="type4" color="1,1,0" begin="0" end="60000" probability="0.05" type="car" route="r_l"/> -->
 
 
 <!-- <personFlow period="10" end="60000" begin="0" id="p1">
